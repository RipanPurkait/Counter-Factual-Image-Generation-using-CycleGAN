{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!mkdir -p ~/.kaggle\n","!cp /kaggle/input/apikaggle/kaggle.json ~/.kaggle/\n","\n","!kaggle competitions download -c rsna-pneumonia-detection-challenge"]},{"cell_type":"markdown","metadata":{},"source":["## Unzip Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#unzip the file\n","import zipfile\n","zip_ref = zipfile.ZipFile('/kaggle/working/rsna-pneumonia-detection-challenge.zip', 'r')\n","zip_ref.extractall('/kaggle/working/rsna-pneumonia-detection-dataset')\n","zip_ref.close()"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:16:49.552419Z","iopub.status.busy":"2024-05-11T08:16:49.552014Z","iopub.status.idle":"2024-05-11T08:17:03.526835Z","shell.execute_reply":"2024-05-11T08:17:03.525649Z","shell.execute_reply.started":"2024-05-11T08:16:49.552388Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (2.4.4)\n","Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.4)\n","Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\n","Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.42.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n","Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\n","Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: torchsummary\n","Successfully installed torchsummary-1.5.1\n"]}],"source":["!pip install pydicom wandb torchsummary"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:17:03.530279Z","iopub.status.busy":"2024-05-11T08:17:03.529342Z","iopub.status.idle":"2024-05-11T08:17:21.168174Z","shell.execute_reply":"2024-05-11T08:17:21.166980Z","shell.execute_reply.started":"2024-05-11T08:17:03.530231Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.4.0-py3-none-any.whl.metadata (19 kB)\n","Collecting lightning==2.2.3\n","  Downloading lightning-2.2.3-py3-none-any.whl.metadata (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m918.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (6.0.1)\n","Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (2024.3.0)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (0.10.1)\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (1.26.4)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (21.3)\n","Requirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (2.1.2)\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (4.66.1)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (4.9.0)\n","Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (2.2.1)\n","Collecting pretty-errors==1.2.25 (from torchmetrics)\n","  Downloading pretty_errors-1.2.25-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from pretty-errors==1.2.25->torchmetrics) (0.4.6)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (3.9.1)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning==2.2.3) (69.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning==2.2.3) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (3.13.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (3.1.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning==2.2.3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning==2.2.3) (1.3.0)\n","Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (3.6)\n","Downloading lightning-2.2.3-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pretty_errors-1.2.25-py3-none-any.whl (17 kB)\n","Installing collected packages: pretty-errors, torchmetrics, lightning\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 1.3.2\n","    Uninstalling torchmetrics-1.3.2:\n","      Successfully uninstalled torchmetrics-1.3.2\n","Successfully installed lightning-2.2.3 pretty-errors-1.2.25 torchmetrics-1.4.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --upgrade torchmetrics lightning==2.2.3"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-11T08:17:21.169857Z","iopub.status.busy":"2024-05-11T08:17:21.169535Z","iopub.status.idle":"2024-05-11T08:17:21.175748Z","shell.execute_reply":"2024-05-11T08:17:21.174875Z","shell.execute_reply.started":"2024-05-11T08:17:21.169825Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import pandas as pd\n","import pydicom\n","from PIL import Image\n","from tqdm import tqdm\n","from concurrent.futures import ProcessPoolExecutor\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.models as models"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:17:21.178899Z","iopub.status.busy":"2024-05-11T08:17:21.177901Z","iopub.status.idle":"2024-05-11T08:17:23.425131Z","shell.execute_reply":"2024-05-11T08:17:23.424115Z","shell.execute_reply.started":"2024-05-11T08:17:21.178871Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"WANDB_KEY\")\n","\n","wandb.login(key=secret_value_0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import pandas as pd\n","import pydicom\n","from PIL import Image\n","from tqdm import tqdm\n","from concurrent.futures import ProcessPoolExecutor\n","\n","class ImageConverter:\n","    def __init__(self, input_folder, output_folder, csv_path, image_size=512):\n","        self.input_folder = input_folder\n","        self.output_folder = output_folder\n","        self.csv_path = csv_path\n","        self.image_size = image_size\n","\n","    def _convert_single_dcm_to_png(self, file_name, label):\n","        # Check if the DICOM file exists\n","        dicom_path = os.path.join(self.input_folder, file_name + '.dcm',)\n","        if not os.path.exists(dicom_path):\n","            print(f\"Warning: DICOM file not found for {dicom_path}\")\n","            return\n","\n","        # Read DICOM file\n","        dicom_data = pydicom.dcmread(dicom_path)\n","\n","        # Convert DICOM to PNG\n","        image_array = dicom_data.pixel_array\n","        image = Image.fromarray(image_array)\n","        image = image.resize((self.image_size, self.image_size))\n","\n","        # Define the output path based on train/test/val and label\n","        if self.index % 5 == 0:  # 20% for validation\n","            output_path = os.path.join(self.output_folder, f'val/{label}/{file_name[:-4]}.png')\n","        elif self.index % 5 == 1:  # 20% for test\n","            output_path = os.path.join(self.output_folder, f'test/{label}/{file_name[:-4]}.png')\n","        else:  # 60% for train\n","            output_path = os.path.join(self.output_folder, f'train/{label}/{file_name[:-4]}.png')\n","\n","        # Save the image\n","        image.save(output_path)\n","\n","    def _convert_dcm_to_png_for_index(self, index_row):\n","        self.index, row = index_row\n","        file_name = row['patientId']\n","        label = row['Target']\n","        self._convert_single_dcm_to_png(file_name, label)\n","\n","    def convert_dcm_to_png_parallel(self):\n","        # Create output folders if they don't exist\n","        for folder in ['train/0/', 'train/1/', 'test/0/', 'test/1/', 'val/0/', 'val/1/']:\n","            os.makedirs(os.path.join(self.output_folder, folder), exist_ok=True)\n","\n","        # Read CSV file\n","        df = pd.read_csv(self.csv_path)\n","        print('Total files: ', df.shape[0])\n","\n","        # Use ProcessPoolExecutor for parallel processing\n","        with ProcessPoolExecutor() as executor:\n","            list(tqdm(executor.map(self._convert_dcm_to_png_for_index, df.iterrows()), total=len(df), desc=\"Converting images\"))\n","# if __name__ == \"__main__\":\n","input_folder = \"/kaggle/working/rsna-pneumonia-detection-dataset/stage_2_train_images\"\n","output_folder = \"/kaggle/working/CycleGan-CFE/train-data\"\n","csv_path = \"/kaggle/working/rsna-pneumonia-detection-dataset/stage_2_train_labels.csv\"\n","\n","image_converter = ImageConverter(input_folder, output_folder, csv_path)\n","image_converter.convert_dcm_to_png_parallel()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:20:47.948688Z","iopub.status.busy":"2024-05-11T08:20:47.947706Z","iopub.status.idle":"2024-05-11T08:20:47.957654Z","shell.execute_reply":"2024-05-11T08:20:47.956735Z","shell.execute_reply.started":"2024-05-11T08:20:47.948637Z"},"trusted":true},"outputs":[],"source":["class ClassifierDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        self.classes = ['0', '1']\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n","\n","        self.samples = self._make_dataset()\n","\n","    def _make_dataset(self):\n","        samples = []\n","        for class_name in self.classes:\n","            class_dir = os.path.join(self.root_dir, class_name)\n","            for img_name in os.listdir(class_dir):\n","                img_path = os.path.join(class_dir, img_name)\n","                samples.append((img_path, self.class_to_idx[class_name]))\n","        return samples\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.samples[idx]\n","        img = Image.open(img_path).convert('L')  # Convert to grayscale\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, label"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:20:49.455414Z","iopub.status.busy":"2024-05-11T08:20:49.454608Z","iopub.status.idle":"2024-05-11T08:20:52.001801Z","shell.execute_reply":"2024-05-11T08:20:52.000913Z","shell.execute_reply.started":"2024-05-11T08:20:49.455380Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Training Images:  16739\n","Total Validation Images:  6046\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import lightning as pl\n","import wandb\n","from lightning.pytorch.loggers.wandb import WandbLogger\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","from lightning.pytorch.tuner import Tuner\n","import tqdm.auto as tqdm\n","from torchmetrics import Accuracy\n","\n","class Classifier(pl.LightningModule):\n","    def __init__(self, transfer=True):\n","        super(Classifier, self).__init__()\n","        self.conv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)  # Adjust input channels to 3\n","        self.model = models.swin_t(weights='IMAGENET1K_V1')\n","        if transfer:\n","            # layers are frozen by using eval()\n","            self.model.eval()\n","            # freeze params\n","            for p in self.model.parameters() : \n","                p.requires_grad = False\n","        num_ftrs = 768\n","        self.model.head = nn.Sequential(\n","            nn.Linear(in_features=num_ftrs, out_features=256),\n","            nn.LeakyReLU(),\n","            nn.Dropout(p=0.5), \n","            nn.Linear(in_features=256 , out_features=2),\n","            nn.Softmax(dim=1)  \n","        ) \n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.train_accuracy = Accuracy(task='binary')\n","        self.val_accuracy = Accuracy(task='binary')\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch\n","        outputs = self(images)\n","        loss = self.criterion(outputs, labels)\n","        self.log('train_loss', loss)\n","        # Calculate and log accuracy\n","        _, preds = torch.max(outputs, 1)\n","        acc = self.train_accuracy(preds, labels)\n","        self.log('train_acc', acc, prog_bar=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        images, labels = batch\n","        outputs = self(images)\n","        loss = self.criterion(outputs, labels)\n","        self.log('val_loss', loss, prog_bar=True, sync_dist=True)\n","        # Calculate and log accuracy\n","        _, preds = torch.max(outputs, 1)\n","        acc = self.val_accuracy(preds, labels)\n","        self.log('val_acc', acc, prog_bar=True, sync_dist=True)\n","        return loss\n","    \n","    def on_train_epoch_end(self):\n","        self.train_accuracy.reset()\n","\n","    def on_validation_epoch_end(self):\n","        self.val_accuracy.reset()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': {\n","                'scheduler': scheduler,\n","                'monitor': 'val_loss',\n","            },\n","            'monitor': 'val_loss'\n","        }\n","\n","wandb_logger = WandbLogger(project=\"CycleGAN-CFE\", name=\"swin_t-classifier-training\")\n","# Define data transformations\n","IMAGE_SIZE = 512\n","BATCH_SIZE = 16\n","EPOCHS = 20\n","\n","transform = transforms.Compose([\n","    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize image to 512x512\n","    transforms.ToTensor(),          \n","    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize image\n","])\n","\n","# Define dataset paths\n","train_dir = \"/kaggle/working/CycleGan-CFE/train-data/train\"\n","val_dir = \"/kaggle/working/CycleGan-CFE/train-data/val\"\n","\n","# Create datasets\n","train_dataset = ClassifierDataset(root_dir=train_dir, transform=transform)\n","val_dataset = ClassifierDataset(root_dir=val_dir, transform=transform)\n","print(\"Total Training Images: \",len(train_dataset))\n","print(\"Total Validation Images: \",len(val_dataset))\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# Instantiate the discriminator model\n","clf = Classifier(transfer=True)\n","\n","checkpoint_callback = ModelCheckpoint(\n","     monitor='val_loss',\n","     dirpath='/kaggle/working/CycleGan-CFE/models/',\n","     filename='swin_t-epoch{epoch:02d}-val_loss{val_loss:.2f}',\n","     auto_insert_metric_name=False,\n"," )\n","# Set up PyTorch Lightning Trainer with multiple GPUs and tqdm progress bar\n","trainer = pl.Trainer(\n","    devices=2,\n","    accelerator=\"gpu\",\n","    max_epochs=EPOCHS,\n","    accumulate_grad_batches=10,\n","    log_every_n_steps=1,\n","    check_val_every_n_epoch=1,\n","    benchmark=True,\n","    logger=wandb_logger,\n","    callbacks=[checkpoint_callback],\n",")\n","\n","\n","# Train the discriminator\n","trainer.fit(clf, train_loader, val_loader)\n","wandb.finish()\n","# if __name__ == \"__main__\":\n","#     main()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:20:54.024842Z","iopub.status.busy":"2024-05-11T08:20:54.024064Z","iopub.status.idle":"2024-05-11T08:20:54.029462Z","shell.execute_reply":"2024-05-11T08:20:54.028082Z","shell.execute_reply.started":"2024-05-11T08:20:54.024804Z"},"trusted":true},"outputs":[],"source":["image_size = 512"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:20:54.685550Z","iopub.status.busy":"2024-05-11T08:20:54.684888Z","iopub.status.idle":"2024-05-11T08:20:54.696825Z","shell.execute_reply":"2024-05-11T08:20:54.695616Z","shell.execute_reply.started":"2024-05-11T08:20:54.685514Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, train_N, train_P, img_res=(128, 128)):\n","        self.root_dir = root_dir\n","        self.train_N = train_N\n","        self.train_P = train_P\n","        self.img_res = img_res\n","        self.transforms = transforms.Compose([\n","            transforms.Resize(img_res),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5], std=[0.5])  # Assuming grayscale images\n","        ])\n","\n","    def __len__(self):\n","        return min(len(os.listdir(os.path.join(self.root_dir, self.train_N))),\n","                   len(os.listdir(os.path.join(self.root_dir, self.train_P))))\n","\n","    def __getitem__(self, idx):\n","        normal_path = os.path.join(self.root_dir, self.train_N, os.listdir(os.path.join(self.root_dir, self.train_N))[idx])\n","        pneumo_path = os.path.join(self.root_dir, self.train_P, os.listdir(os.path.join(self.root_dir, self.train_P))[idx])\n","        \n","        normal_img = Image.open(normal_path).convert(\"L\")  # Load as grayscale\n","        pneumo_img = Image.open(pneumo_path).convert(\"L\")  # Load as grayscale\n","        \n","        normal_img = self.transforms(normal_img)\n","        pneumo_img = self.transforms(pneumo_img)\n","        \n","        return normal_img, pneumo_img\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:20:55.859464Z","iopub.status.busy":"2024-05-11T08:20:55.858626Z","iopub.status.idle":"2024-05-11T08:20:55.886663Z","shell.execute_reply":"2024-05-11T08:20:55.885765Z","shell.execute_reply.started":"2024-05-11T08:20:55.859426Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import lightning as pl\n","import wandb\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ResUNetGenerator(pl.LightningModule):\n","    def __init__(self, gf, channels, dropout_rate=0.3):\n","        super(ResUNetGenerator, self).__init__()\n","        self.gf = gf\n","        self.channels = channels\n","        self.dropout_rate = dropout_rate\n","\n","        # Define the layers for the encoder\n","        self.conv2d = nn.Conv2d(channels, gf, kernel_size=4, stride=2, padding=1, padding_mode='reflect')\n","        \n","        self.conv2d_layers_left = nn.ModuleList([\n","            nn.Conv2d(gf * 2**i, gf * 2**(i+1), kernel_size=4, stride=2, padding=1, padding_mode='reflect')\n","            for i in range(4)\n","        ])\n","        \n","        self.conv2d_layers_right = nn.ModuleList([\n","            nn.Conv2d(gf * 2**(i+1), gf * 2**i, kernel_size=3, stride=1, padding=1, padding_mode='reflect')\n","            for i in range(4)\n","        ])\n","        \n","        self.groupNorm_layers = nn.ModuleList([\n","            nn.GroupNorm(8, gf * 2**(i+1))\n","            for i in range(4)\n","        ])\n","        \n","        self.res_blocks_left = nn.ModuleList([\n","            ResidualBlock(gf * 2**(i+1), gf * 2**(i+1), dropout_rate)\n","            for i in range(4)\n","        ])\n","        \n","        self.res_blocks_right = nn.ModuleList([\n","            ResidualBlock(gf * 2**i, gf * 2**i, dropout_rate)\n","            for i in range(4)\n","        ])\n","\n","        # Define the layers for the decoder\n","        self.deconv2d_layers = nn.ModuleList([\n","            nn.ConvTranspose2d(gf * 2**(4-i), gf * 2**(3-i), kernel_size=4, stride=2, padding=1)\n","            for i in range(4)\n","        ])\n","        self.deconv2d_final = nn.ConvTranspose2d(gf, channels, kernel_size=4, stride=2, padding=1)\n","        self.leaky_relu = nn.LeakyReLU(0.2)\n","        self.group_norm = nn.GroupNorm(8, gf)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \n","        d0 = self.leaky_relu(self.group_norm(self.conv2d(x)))\n","        d1 = self.leaky_relu(self.groupNorm_layers[0](self.conv2d_layers_left[0](d0)))\n","        d1 = self.res_blocks_left[0](d1)\n","        \n","        d2 = self.leaky_relu(self.groupNorm_layers[1](self.conv2d_layers_left[1](d1)))\n","        d2 = self.res_blocks_left[1](d2)\n","        \n","        d3 = self.leaky_relu(self.groupNorm_layers[2](self.conv2d_layers_left[2](d2)))\n","        d3 = self.res_blocks_left[2](d3)\n","        \n","        d4 = self.leaky_relu(self.groupNorm_layers[3](self.conv2d_layers_left[3](d3)))\n","        d4 = self.res_blocks_left[3](d4)\n","\n","\n","        # Decoder\n","        u1 = self.deconv2d_layers[0](d4)\n","        u1 = torch.cat((u1, d3), dim=1)\n","        u1 = self.leaky_relu(self.groupNorm_layers[2](self.conv2d_layers_right[3](u1)))\n","        u1 = self.res_blocks_right[3](u1)\n","        \n","        u2 = self.deconv2d_layers[1](u1)\n","        u2 = torch.cat((u2, d2), dim=1)\n","        u2 = self.leaky_relu(self.groupNorm_layers[1](self.conv2d_layers_right[2](u2)))\n","        u2 = self.res_blocks_right[2](u2)\n","\n","        u3 = self.deconv2d_layers[2](u2)\n","        u3 = torch.cat((u3, d1), dim=1)\n","        u3 = self.leaky_relu(self.groupNorm_layers[0](self.conv2d_layers_right[1](u3)))\n","        u3 = self.res_blocks_right[1](u3)\n","        \n","        u4 = self.deconv2d_layers[3](u3)\n","        u4 = torch.cat((u4, d0), dim=1)\n","        u4 = self.leaky_relu(self.group_norm(self.conv2d_layers_right[0](u4)))\n","        u4 = self.res_blocks_right[0](u4)\n","\n","        output_img = self.sig(self.deconv2d_final(u4))\n","\n","        return output_img\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        return optimizer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:20:57.264569Z","iopub.status.busy":"2024-05-11T08:20:57.264201Z","iopub.status.idle":"2024-05-11T08:20:57.273708Z","shell.execute_reply":"2024-05-11T08:20:57.272578Z","shell.execute_reply.started":"2024-05-11T08:20:57.264540Z"},"trusted":true},"outputs":[],"source":["class Discriminator(pl.LightningModule):\n","    def __init__(self, df):\n","        super(Discriminator, self).__init__()\n","        self.df = df\n","        # Define the layers for the discriminator\n","        self.conv_layers = nn.ModuleList([nn.Sequential(\n","            nn.Conv2d(1 if i == 0 else df * 2**(i-1), df * 2**i, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.GroupNorm(8, df * 2**i)) for i in range(4)])\n","        \n","        self.final_conv = nn.Conv2d(df * 8, 1, kernel_size=4, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        out = x\n","        for conv_layer in self.conv_layers:\n","            out = conv_layer(out)\n","        validity = self.final_conv(out)\n","        return validity\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        return optimizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from torchsummary import summary\n","model = Discriminator(df=64)\n","# Summarize the model architecture\n","summary(model, input_size=(1, 512, 512))"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from torchsummary import summary\n","model = ResUNetGenerator(gf=32, channels=1)\n","# Summarize the model architecture\n","summary(model, input_size=(1, 512, 512))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:21:03.316410Z","iopub.status.busy":"2024-05-11T08:21:03.316016Z","iopub.status.idle":"2024-05-11T08:21:03.321864Z","shell.execute_reply":"2024-05-11T08:21:03.320555Z","shell.execute_reply.started":"2024-05-11T08:21:03.316377Z"},"trusted":true},"outputs":[],"source":["batch_size = 4"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:22:22.929316Z","iopub.status.busy":"2024-05-11T08:22:22.928831Z","iopub.status.idle":"2024-05-11T08:22:22.988941Z","shell.execute_reply":"2024-05-11T08:22:22.987749Z","shell.execute_reply.started":"2024-05-11T08:22:22.929281Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class CycleGAN(pl.LightningModule):\n","    def __init__(self, img_shape=(1, 512, 512), gf=32, df=64, lambda_cycle=10.0, lambda_id=0.1, classifier_path=None, classifier_weight=None):\n","        super(CycleGAN, self).__init__()\n","        self.img_shape = img_shape\n","        self.gf = gf\n","        self.df = df\n","        self.lambda_cycle = lambda_cycle\n","        self.lambda_id = lambda_id * lambda_cycle\n","        self.classifier_path = classifier_path\n","        self.classifier_weight = classifier_weight\n","\n","        # Initialize the generator, discriminator, and classifier models\n","        self.g_NP = ResUNetGenerator(gf, channels=self.img_shape[0])\n","        self.g_PN = ResUNetGenerator(gf, channels=self.img_shape[0])\n","        self.d_N = Discriminator(df)\n","        self.d_P = Discriminator(df)\n","        self.automatic_optimization = False\n","        \n","        self.classifier = Classifier()\n","        checkpoint = torch.load(classifier_path)\n","        self.classifier.load_state_dict(checkpoint['state_dict'])\n","        self.classifier.eval()\n","        self.freeze_classifier()\n","    \n","    def freeze_classifier(self):\n","        print(\"freezing Classifier...\")\n","        for p in self.classifier.parameters() : \n","                p.requires_grad = False\n","\n","\n","    def generator_training_step(self, img_N, img_P, opt):\n","        self.toggle_optimizer(opt)\n","        # Translate images to the other domain\n","        fake_P = self.g_NP(img_N)\n","        fake_N = self.g_PN(img_P)\n","\n","        # Translate images back to original domain\n","        reconstr_N = self.g_PN(fake_P)\n","        reconstr_P = self.g_NP(fake_N)\n","\n","        # Identity mapping of images\n","        img_N_id = self.g_PN(img_N)\n","        img_P_id = self.g_NP(img_P)\n","        # Discriminators determine validity of translated images\n","        valid_N = self.d_N(fake_N)\n","        valid_P = self.d_P(fake_P)\n","\n","        class_N_loss = self.classifier(fake_N)\n","        class_P_loss = self.classifier(fake_P)\n","        # Adversarial loss\n","        valid_target = torch.ones_like(valid_N)\n","        adversarial_loss = nn.MSELoss()(valid_N, valid_target) + nn.MSELoss()(valid_P, valid_target)\n","\n","        # Cycle consistency loss\n","        cycle_loss = nn.L1Loss()(reconstr_N, img_N) + nn.L1Loss()(reconstr_P, img_P)\n","\n","        # Identity loss\n","        identity_loss = nn.L1Loss()(img_N_id, img_N) + nn.L1Loss()(img_P_id, img_P)\n","\n","        # Classifier loss\n","        class_loss = nn.MSELoss()(class_N_loss, torch.ones_like(class_N_loss)) + nn.MSELoss()(class_P_loss, torch.zeros_like(class_P_loss))\n","\n","        # Total generator loss\n","        total_loss = adversarial_loss + self.lambda_cycle * cycle_loss + self.lambda_id * identity_loss + self.classifier_weight * class_loss\n","              \n","        self.log('adversarial_loss', adversarial_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('cycle_loss', cycle_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('identity_loss', identity_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('class_loss', class_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('generator_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        \n","        opt.zero_grad()\n","        self.manual_backward(total_loss)\n","        opt.step()\n","        self.untoggle_optimizer(opt)\n","        \n","        return total_loss, adversarial_loss, cycle_loss\n","\n","    def discriminator_training_step(self, img_N, img_P, opt):\n","        # Pass real images through discriminator D_N\n","        self.toggle_optimizer(opt)\n","        pred_real_N = self.d_N(img_N)\n","        # Compute MSE loss for real Negative images\n","        mse_real_N = nn.MSELoss()(pred_real_N, torch.ones_like(pred_real_N))\n","\n","        # Pass fake images from positive to discriminator D_N\n","        fake_P = self.g_PN(img_P)\n","        pred_fake_N = self.d_N(fake_P)\n","        # Compute MSE loss for fake images in domain P\n","        mse_fake_N = nn.MSELoss()(pred_fake_N, torch.zeros_like(pred_fake_N))\n","        # Pass real images through discriminator D_P\n","        pred_real_P = self.d_P(img_P)\n","        # Compute MSE loss for real images in domain P\n","        mse_real_P = nn.MSELoss()(pred_real_P, torch.ones_like(pred_real_P))\n","\n","        # Pass fake images from domain N to discriminator D_P\n","        fake_N = self.g_NP(img_N)  # Detach to prevent backpropagation to generator\n","        pred_fake_P = self.d_P(fake_N)\n","        # Compute MSE loss for fake images in domain N\n","        mse_fake_P = nn.MSELoss()(pred_fake_P, torch.zeros_like(pred_fake_P))\n","        \n","\n","        # Compute total discriminator loss\n","        dis_loss = 0.5 * (mse_real_N + mse_fake_N + mse_real_P + mse_fake_P)\n","        opt.zero_grad()\n","        self.manual_backward(mse_fake_P)\n","        opt.step()\n","        self.untoggle_optimizer(opt)\n","        \n","        self.log('mse_fake_N', mse_fake_N, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('mse_fake_P', mse_fake_P, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('discriminator_loss', dis_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","\n","        return dis_loss, mse_fake_N, mse_fake_P\n","    \n","    def training_step(self, batch, batch_idx):\n","        img_N, img_P = batch\n","        optD, optG = self.optimizers()\n","        \n","        total_loss, adversarial_loss, cycle_loss = self.generator_training_step(img_N, img_P, optG)\n","        dis_loss, mse_fake_N, mse_fake_P = self.discriminator_training_step(img_N, img_P, optD) \n","        \n","        return {\"generator_loss\": total_loss, \"adversarial_loss\": adversarial_loss, \"cycle_loss\": cycle_loss, \"discriminator_loss\": dis_loss, \"mse_fake_N\": mse_fake_N, \"mse_fake_P\": mse_fake_P}\n","    \n","    def validation_step(self, batch, batch_idx):\n","        img_N, img_P = batch\n","\n","        # Translate images to the other domain\n","        fake_P = self.g_NP(img_N)\n","        fake_N = self.g_PN(img_P)\n","\n","        # Translate images back to original domain\n","        reconstr_N = self.g_PN(fake_P)\n","        reconstr_P = self.g_NP(fake_N)\n","\n","        # Identity mapping of images\n","        img_N_id = self.g_PN(img_N)\n","        img_P_id = self.g_NP(img_P)\n","\n","        # Discriminators determine validity of translated images\n","        valid_N = self.d_N(fake_N)\n","        valid_P = self.d_P(fake_P)\n","\n","        class_N_loss = self.classifier(fake_N)\n","        class_P_loss = self.classifier(fake_P)\n","\n","        # Adversarial loss\n","        valid_target = torch.ones_like(valid_N)\n","        adversarial_loss = nn.MSELoss()(valid_N, valid_target) + nn.MSELoss()(valid_P, valid_target)\n","\n","        # Cycle consistency loss\n","        cycle_loss = nn.L1Loss()(reconstr_N, img_N) + nn.L1Loss()(reconstr_P, img_P)\n","\n","        # Identity loss\n","        identity_loss = nn.L1Loss()(img_N_id, img_N) + nn.L1Loss()(img_P_id, img_P)\n","\n","        # Classifier loss\n","        class_loss = nn.MSELoss()(class_N_loss, torch.ones_like(class_N_loss)) + nn.MSELoss()(class_P_loss, torch.zeros_like(class_P_loss))\n","\n","        # Total generator loss\n","        total_loss = adversarial_loss + self.lambda_cycle * cycle_loss + self.lambda_id * identity_loss + self.classifier_weight * class_loss\n","\n","        self.log('val_adversarial_loss', adversarial_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_cycle_loss', cycle_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_identity_loss', identity_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_class_loss', class_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log('val_generator_loss', total_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","        return total_loss\n","\n","    def configure_optimizers(self):\n","        optG = torch.optim.Adam(itertools.chain(self.g_NP.parameters(), self.g_PN.parameters()),lr=2e-4, betas=(0.5, 0.999))\n","        optD = torch.optim.Adam(itertools.chain(self.d_N.parameters(), self.d_P.parameters()),lr=2e-4, betas=(0.5, 0.999))\n","        \n","        gamma = lambda epoch: 1 - max(0, epoch + 1 - 100) / 101\n","        schD = LambdaLR(optD, lr_lambda=gamma)\n","#         Optimizer= [optD, optG]\n","        return optD, optG\n","\n","    def train_dataloader(self):\n","        root_dir = \"/kaggle/working/CycleGan-CFE/train-data/train\"\n","        train_N = \"0\"\n","        train_P = \"1\"\n","        img_res = (image_size, image_size)\n","\n","        dataset = CustomDataset(root_dir=root_dir, train_N=train_N, train_P=train_P, img_res=img_res)\n","\n","        # Set up DataLoader for parallel processing and GPU acceleration\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","\n","        return dataloader\n","    \n","    def val_dataloader(self):\n","        root_dir = \"/kaggle/working/CycleGan-CFE/train-data/val\"\n","        train_N = \"0\"\n","        train_P = \"1\"\n","        img_res = (image_size, image_size)\n","\n","        dataset = CustomDataset(root_dir=root_dir, train_N=train_N, train_P=train_P, img_res=img_res)\n","\n","        # Set up DataLoader for parallel processing and GPU acceleration\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","        return dataloader\n","     \n","\n","    def on_train_batch_end(self, outputs, batch, batch_idx):\n","        if batch_idx % 100 == 0:\n","            # Get a random batch from the test dataloader\n","            batch = next(iter(test_dataloader))\n","            img_N, img_P = batch\n","\n","            # Pick a random image from the batch\n","            idx = np.random.randint(img_N.size(0))\n","            img_N = img_N[idx].unsqueeze(0).to('cuda')\n","            img_P = img_P[idx].unsqueeze(0).to('cuda')\n","            # Translate images to the other domain\n","            fake_P = self.g_NP(img_N)\n","            fake_N = self.g_PN(img_P)\n","\n","            # Translate images back to original domain\n","            reconstr_N = self.g_PN(fake_P)\n","            reconstr_P = self.g_NP(fake_N)\n","\n","            # Plot the images\n","            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n","\n","            # Plot real N, translated P, and reconstructed N\n","            axes[0, 0].imshow(img_N.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n","            axes[0, 0].set_title(\"Real N\")\n","            axes[0, 0].axis('off')\n","\n","            axes[0, 1].imshow(fake_P.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n","            axes[0, 1].set_title(\"Translated P\")\n","            axes[0, 1].axis('off')\n","\n","            axes[0, 2].imshow(reconstr_N.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n","            axes[0, 2].set_title(\"Reconstructed N\")\n","            axes[0, 2].axis('off')\n","\n","            # Plot real P, translated N, and reconstructed P\n","            axes[1, 0].imshow(img_P.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n","            axes[1, 0].set_title(\"Real P\")\n","            axes[1, 0].axis('off')\n","\n","            axes[1, 1].imshow(fake_N.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n","            axes[1, 1].set_title(\"Translated N\")\n","            axes[1, 1].axis('off')\n","\n","            axes[1, 2].imshow(reconstr_P.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n","            axes[1, 2].set_title(\"Reconstructed P\")\n","            axes[1, 2].axis('off')\n","\n","            # Log the figure in WandB\n","            wandb.log({\"test_images\": wandb.Image(fig)})\n","\n","            plt.close(fig)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-11T08:25:10.375894Z","iopub.status.busy":"2024-05-11T08:25:10.375474Z","iopub.status.idle":"2024-05-11T08:30:15.200589Z","shell.execute_reply":"2024-05-11T08:30:15.199330Z","shell.execute_reply.started":"2024-05-11T08:25:10.375862Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["freezing Classifier.\n"]},{"name":"stderr","output_type":"stream","text":["INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n","INFO: GPU available: True (cuda), used: True\n","INFO: TPU available: False, using: 0 TPU cores\n","INFO: IPU available: False, using: 0 IPUs\n","INFO: HPU available: False, using: 0 HPUs\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>./wandb/run-20240511_082511-1pxj8b6d</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE/runs/1pxj8b6d' target=\"_blank\">GAN-training</a></strong> to <a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE' target=\"_blank\">https://wandb.ai/anindyamitra2018/CycleGAN-CFE</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE/runs/1pxj8b6d' target=\"_blank\">https://wandb.ai/anindyamitra2018/CycleGAN-CFE/runs/1pxj8b6d</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working/CycleGan-CFE/models exists and is not empty.\n","INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n","INFO: \n","  | Name       | Type             | Params\n","------------------------------------------------\n","0 | g_NP       | ResUNetGenerator | 15.0 M\n","1 | g_PN       | ResUNetGenerator | 15.0 M\n","2 | d_N        | Discriminator    | 2.8 M \n","3 | d_P        | Discriminator    | 2.8 M \n","4 | classifier | Classifier       | 27.7 M\n","------------------------------------------------\n","35.5 M    Trainable params\n","27.7 M    Non-trainable params\n","63.2 M    Total params\n","252.837   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"467c1f9b54924932bd797b8de98332bc","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"]}],"source":["from torch.optim.lr_scheduler import LambdaLR\n","import itertools\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","\n","cyclegan = CycleGAN(gf=32, df=64, classifier_path='/kaggle/input/swin-tiny/pytorch/v0/1/swin_t-epoch00-val_loss0.35.ckpt', classifier_weight=1)\n","\n","checkpoint_callback = ModelCheckpoint(dirpath=\"/kaggle/working/CycleGan-CFE/models\",\n","                                      filename='cyclegan-epoch_{epoch}-vloss_{val_generator_loss:.2f}.ckpt',\n","                                      monitor='val_generator_loss',\n","                                      save_top_k=3,\n","                                      save_last=True,\n","                                      save_weights_only=True,\n","                                      verbose=True,\n","                                      mode='min')\n","\n","testdata_dir = \"/kaggle/working/CycleGan-CFE/train-data/val\"\n","train_N = \"0\"\n","train_P = \"1\"\n","img_res = (image_size, image_size)\n","\n","test_dataset = CustomDataset(root_dir=testdata_dir, train_N=train_N, train_P=train_P, img_res=img_res)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","wandb_logger = WandbLogger(project=\"CycleGAN-CFE\", name=\"GAN-training\",log_model=\"all\")\n","# Create the trainer\n","trainer = pl.Trainer(\n","    accelerator=\"auto\",\n","    max_epochs=2,\n","    log_every_n_steps=1,\n","    benchmark=True,\n","    devices=\"auto\",\n","    logger=wandb_logger,\n","    callbacks= [checkpoint_callback]\n",")\n","\n","# Train the CycleGAN model\n","trainer.fit(cyclegan)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pl.__version__"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 512, 16, 16])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class AttentionGate(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(AttentionGate, self).__init__()\n","        self.conv_gate = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.conv_x = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x, g):\n","        gate = self.conv_gate(g)\n","        x = self.conv_x(x)\n","        attention = self.softmax(gate)\n","        x_att = x * attention\n","        return x_att\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Down, self).__init__()\n","        self.conv = DoubleConv(in_channels, out_channels)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x_pool = self.pool(x)\n","        return x, x_pool\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Up, self).__init__()\n","        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","        self.attention = AttentionGate(in_channels // 2, out_channels)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        x2 = self.attention(x2, x1)\n","        x = torch.cat([x1, x2], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","class AttentionUNetGenerator(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=1):\n","        super(AttentionUNetGenerator, self).__init__()\n","        self.down1 = Down(in_channels, 64)\n","        self.down2 = Down(64, 128)\n","        self.down3 = Down(128, 256)\n","        self.down4 = Down(256, 512)\n","        self.up1 = Up(512, 256)\n","        self.up2 = Up(512, 128)\n","        self.up3 = Up(256, 64)\n","        self.up4 = nn.Conv2d(128, out_channels, kernel_size=1, stride=1)\n","\n","    def forward(self, x):\n","        x1, x = self.down1(x)\n","        x2, x = self.down2(x)\n","        x3, x = self.down3(x)\n","        x4, x = self.down4(x)\n","        x = self.up1(x4, x3)\n","        x = self.up2(x, x2)\n","        x = self.up3(x, x1)\n","        x = self.up4(x)\n","        return x\n","    \n","# Example usage\n","generator = AttentionGate(in_channels=512, out_channels=512)\n","input_tensor = torch.randn(1, 512, 16, 16)\n","g = torch.randn(1, 512, 16, 16)\n","output_tensor = generator(input_tensor, g)\n","print(output_tensor.shape)  # Output: torch.Size([1, 1, 512, 512])"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 256, 256]             544\n","         LeakyReLU-2         [-1, 32, 256, 256]               0\n","         GroupNorm-3         [-1, 32, 256, 256]              64\n","            Conv2d-4         [-1, 64, 128, 128]          32,832\n","         LeakyReLU-5         [-1, 64, 128, 128]               0\n","         GroupNorm-6         [-1, 64, 128, 128]             128\n","            Conv2d-7          [-1, 128, 64, 64]         131,200\n","         LeakyReLU-8          [-1, 128, 64, 64]               0\n","         GroupNorm-9          [-1, 128, 64, 64]             256\n","           Conv2d-10          [-1, 256, 32, 32]         524,544\n","        LeakyReLU-11          [-1, 256, 32, 32]               0\n","        GroupNorm-12          [-1, 256, 32, 32]             512\n","  ConvTranspose2d-13          [-1, 128, 64, 64]         524,416\n","             ReLU-14          [-1, 128, 64, 64]               0\n","        GroupNorm-15          [-1, 128, 64, 64]             256\n","           Conv2d-16          [-1, 256, 64, 64]          33,024\n","           Conv2d-17          [-1, 256, 64, 64]          33,024\n","          Softmax-18          [-1, 256, 64, 64]               0\n","    AttentionGate-19          [-1, 256, 64, 64]               0\n","  ConvTranspose2d-20         [-1, 64, 128, 128]         262,208\n","             ReLU-21         [-1, 64, 128, 128]               0\n","        GroupNorm-22         [-1, 64, 128, 128]             128\n","           Conv2d-23        [-1, 128, 128, 128]           8,320\n","           Conv2d-24        [-1, 128, 128, 128]           8,320\n","          Softmax-25        [-1, 128, 128, 128]               0\n","    AttentionGate-26        [-1, 128, 128, 128]               0\n","  ConvTranspose2d-27         [-1, 32, 256, 256]          65,568\n","             ReLU-28         [-1, 32, 256, 256]               0\n","        GroupNorm-29         [-1, 32, 256, 256]              64\n","           Conv2d-30         [-1, 64, 256, 256]           2,112\n","           Conv2d-31         [-1, 64, 256, 256]           2,112\n","          Softmax-32         [-1, 64, 256, 256]               0\n","    AttentionGate-33         [-1, 64, 256, 256]               0\n","  ConvTranspose2d-34          [-1, 1, 512, 512]           1,025\n","             Tanh-35          [-1, 1, 512, 512]               0\n","================================================================\n","Total params: 1,630,657\n","Trainable params: 1,630,657\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.00\n","Forward/backward pass size (MB): 402.00\n","Params size (MB): 6.22\n","Estimated Total Size (MB): 409.22\n","----------------------------------------------------------------\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class AttentionGate(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(AttentionGate, self).__init__()\n","        self.conv_gate = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.conv_x = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x, g):\n","        gate = self.conv_gate(g)\n","        x = self.conv_x(x)\n","        attention = self.softmax(gate)\n","        x_att = x * attention\n","        return x_att\n","    \n","class UNetGenerator(nn.Module):\n","    def __init__(self, img_shape, gf, channels):\n","        super(UNetGenerator, self).__init__()\n","        self.img_shape = img_shape\n","        self.channels = channels\n","        \n","        # Downsampling layers\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(channels, gf, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(gf, gf * 2, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf * 2)\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(gf * 2, gf * 4, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf * 4)\n","        )\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(gf * 4, gf * 8, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf * 8)\n","        )\n","\n","        self.attn_layer = nn.ModuleList([\n","            AttentionGate(gf * 2**(i), gf * 2**(i+1))\n","            for i in range(3)\n","        ])\n","\n","        # Upsampling layers\n","        self.deconv1 = nn.Sequential(\n","            nn.ConvTranspose2d(gf * 8, gf * 4, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf * 4)\n","        )\n","        self.deconv2 = nn.Sequential(\n","            nn.ConvTranspose2d(gf * 8, gf * 2, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf * 2)\n","        )\n","        self.deconv3 = nn.Sequential(\n","            nn.ConvTranspose2d(gf * 4, gf, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.GroupNorm(num_groups=1, num_channels=gf)\n","        )\n","        self.deconv4 = nn.Sequential(\n","            nn.ConvTranspose2d(gf * 2, channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh()\n","        )\n","        \n","    def forward(self, x):\n","        # Downsampling\n","        d1 = self.conv1(x)\n","        d2 = self.conv2(d1)\n","        d3 = self.conv3(d2)\n","        d4 = self.conv4(d3)\n","        \n","        # Upsampling\n","        u1 = self.deconv1(d4)\n","        u1 = self.attn_layer[2](d3, u1)\n","        \n","        u2 = self.deconv2(u1)\n","        u2 = self.attn_layer[1](d2, u2)\n","        \n","        u3 = self.deconv3(u2)\n","        u3 = self.attn_layer[0](d1, u3)\n","        \n","        output = self.deconv4(u3)\n","        \n","        return output\n","\n","model = UNetGenerator(img_shape=(1, 512, 512), gf=32, channels=1)\n","summary(model=model, input_size=(1, 512, 512))"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import lightning as pl\n","import wandb\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","    \n","class AttentionGate(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(AttentionGate, self).__init__()\n","        self.conv_gate = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.conv_x = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x, g):\n","        gate = self.conv_gate(g)\n","        x = self.conv_x(x)\n","        attention = self.softmax(gate)\n","        x_att = x * attention\n","        return x_att\n","    \n","\n","class ResUNetGenerator(pl.LightningModule):\n","    def __init__(self, gf, channels, dropout_rate=0.3):\n","        super(ResUNetGenerator, self).__init__()\n","        self.gf = gf\n","        self.channels = channels\n","        self.dropout_rate = dropout_rate\n","\n","        # Define the layers for the encoder\n","        self.conv2d = nn.Conv2d(channels, gf, kernel_size=4, stride=2, padding=1, padding_mode='reflect')\n","        \n","        self.conv2d_layers_left = nn.ModuleList([\n","            nn.Conv2d(gf * 2**i, gf * 2**(i+1), kernel_size=4, stride=2, padding=1, padding_mode='reflect')\n","            for i in range(3)\n","        ])\n","        \n","        self.attn_layer = nn.ModuleList([\n","            AttentionGate(gf * 2**(i), gf * 2**(i))\n","            for i in range(3)\n","        ])\n","        \n","        self.groupNorm_layers = nn.ModuleList([\n","            nn.GroupNorm(8, gf * 2**(i+1))\n","            for i in range(3)\n","        ])\n","        \n","        self.res_blocks_left = nn.ModuleList([\n","            ResidualBlock(gf * 2**(i+1), gf * 2**(i+1), dropout_rate)\n","            for i in range(3)\n","        ])\n","        \n","\n","\n","        # Define the layers for the decoder\n","        self.deconv2d_layers = nn.ModuleList([\n","            nn.ConvTranspose2d(gf * 2**(3-i), gf * 2**(2-i), kernel_size=4, stride=2, padding=1)\n","            for i in range(3)\n","        ])\n","\n","        self.deconv2d_final = nn.ConvTranspose2d(gf, channels, kernel_size=4, stride=2, padding=1)\n","        self.leaky_relu = nn.LeakyReLU(0.2)\n","        self.group_norm = nn.GroupNorm(8, gf)\n","        self.sig = nn.Tanh()\n","\n","    def forward(self, x):\n","        \n","        d0 = self.leaky_relu(self.group_norm(self.conv2d(x)))\n","        d1 = self.leaky_relu(self.groupNorm_layers[0](self.conv2d_layers_left[0](d0)))\n","        d1 = self.res_blocks_left[0](d1)\n","        \n","        d2 = self.leaky_relu(self.groupNorm_layers[1](self.conv2d_layers_left[1](d1)))\n","        d2 = self.res_blocks_left[1](d2)\n","        \n","        d3 = self.leaky_relu(self.groupNorm_layers[2](self.conv2d_layers_left[2](d2)))\n","        d3 = self.res_blocks_left[2](d3)\n","    \n","        \n","        u1 = self.deconv2d_layers[0](d3)\n","        u1 = self.attn_layer[2](u1, d2)\n","\n","        u2 = self.deconv2d_layers[1](u1)\n","        u2 = self.attn_layer[1](u2, d1)\n","        \n","        u3 = self.deconv2d_layers[2](u2)\n","        u3 = self.attn_layer[0](u3, d0)\n","\n","        output_img = self.sig(self.deconv2d_final(u3))\n","\n","        return output_img\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","        return optimizer"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["from torchsummary import summary"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 256, 256]             544\n","         GroupNorm-2         [-1, 32, 256, 256]              64\n","         LeakyReLU-3         [-1, 32, 256, 256]               0\n","            Conv2d-4         [-1, 64, 128, 128]          32,832\n","         GroupNorm-5         [-1, 64, 128, 128]             128\n","         LeakyReLU-6         [-1, 64, 128, 128]               0\n","            Conv2d-7         [-1, 64, 128, 128]          36,864\n","              ReLU-8         [-1, 64, 128, 128]               0\n","            Conv2d-9         [-1, 64, 128, 128]          36,864\n","             ReLU-10         [-1, 64, 128, 128]               0\n","    ResidualBlock-11         [-1, 64, 128, 128]               0\n","           Conv2d-12          [-1, 128, 64, 64]         131,200\n","        GroupNorm-13          [-1, 128, 64, 64]             256\n","        LeakyReLU-14          [-1, 128, 64, 64]               0\n","           Conv2d-15          [-1, 128, 64, 64]         147,456\n","             ReLU-16          [-1, 128, 64, 64]               0\n","           Conv2d-17          [-1, 128, 64, 64]         147,456\n","             ReLU-18          [-1, 128, 64, 64]               0\n","    ResidualBlock-19          [-1, 128, 64, 64]               0\n","           Conv2d-20          [-1, 256, 32, 32]         524,544\n","        GroupNorm-21          [-1, 256, 32, 32]             512\n","        LeakyReLU-22          [-1, 256, 32, 32]               0\n","           Conv2d-23          [-1, 256, 32, 32]         589,824\n","             ReLU-24          [-1, 256, 32, 32]               0\n","           Conv2d-25          [-1, 256, 32, 32]         589,824\n","             ReLU-26          [-1, 256, 32, 32]               0\n","    ResidualBlock-27          [-1, 256, 32, 32]               0\n","  ConvTranspose2d-28          [-1, 128, 64, 64]         524,416\n","           Conv2d-29          [-1, 128, 64, 64]          16,512\n","           Conv2d-30          [-1, 128, 64, 64]          16,512\n","          Softmax-31          [-1, 128, 64, 64]               0\n","    AttentionGate-32          [-1, 128, 64, 64]               0\n","  ConvTranspose2d-33         [-1, 64, 128, 128]         131,136\n","           Conv2d-34         [-1, 64, 128, 128]           4,160\n","           Conv2d-35         [-1, 64, 128, 128]           4,160\n","          Softmax-36         [-1, 64, 128, 128]               0\n","    AttentionGate-37         [-1, 64, 128, 128]               0\n","  ConvTranspose2d-38         [-1, 32, 256, 256]          32,800\n","           Conv2d-39         [-1, 32, 256, 256]           1,056\n","           Conv2d-40         [-1, 32, 256, 256]           1,056\n","          Softmax-41         [-1, 32, 256, 256]               0\n","    AttentionGate-42         [-1, 32, 256, 256]               0\n","  ConvTranspose2d-43          [-1, 1, 512, 512]             513\n","             Tanh-44          [-1, 1, 512, 512]               0\n","================================================================\n","Total params: 2,970,689\n","Trainable params: 2,970,689\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.00\n","Forward/backward pass size (MB): 304.00\n","Params size (MB): 11.33\n","Estimated Total Size (MB): 316.33\n","----------------------------------------------------------------\n"]}],"source":["model = ResUNetGenerator(gf=32, channels=1)\n","summary(model=model, input_size=(1, 512, 512))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4336295,"sourceId":7449605,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":38769,"sourceId":46250,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":38771,"sourceId":46253,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
