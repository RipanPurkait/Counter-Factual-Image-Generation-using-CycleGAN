{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7449605,"sourceType":"datasetVersion","datasetId":4336295},{"sourceId":46250,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":38769},{"sourceId":46253,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":38771}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!cp /kaggle/input/apikaggle/kaggle.json ~/.kaggle/\n\n!kaggle competitions download -c rsna-pneumonia-detection-challenge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unzip Dataset","metadata":{}},{"cell_type":"code","source":"#unzip the file\nimport zipfile\nzip_ref = zipfile.ZipFile('/kaggle/working/rsna-pneumonia-detection-challenge.zip', 'r')\nzip_ref.extractall('/kaggle/working/rsna-pneumonia-detection-dataset')\nzip_ref.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pydicom wandb torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:16:49.552014Z","iopub.execute_input":"2024-05-11T08:16:49.552419Z","iopub.status.idle":"2024-05-11T08:17:03.526835Z","shell.execute_reply.started":"2024-05-11T08:16:49.552388Z","shell.execute_reply":"2024-05-11T08:17:03.525649Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (2.4.4)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.4)\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.42.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install --upgrade torchmetrics lightning==2.2.3","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:17:03.529342Z","iopub.execute_input":"2024-05-11T08:17:03.530279Z","iopub.status.idle":"2024-05-11T08:17:21.168174Z","shell.execute_reply.started":"2024-05-11T08:17:03.530231Z","shell.execute_reply":"2024-05-11T08:17:21.166980Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nCollecting torchmetrics\n  Downloading torchmetrics-1.4.0-py3-none-any.whl.metadata (19 kB)\nCollecting lightning==2.2.3\n  Downloading lightning-2.2.3-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m918.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (2024.3.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (0.10.1)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (2.1.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.3) (2.2.1)\nCollecting pretty-errors==1.2.25 (from torchmetrics)\n  Downloading pretty_errors-1.2.25-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from pretty-errors==1.2.25->torchmetrics) (0.4.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning==2.2.3) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning==2.2.3) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.3) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning==2.2.3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning==2.2.3) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.3) (3.6)\nDownloading lightning-2.2.3-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pretty_errors-1.2.25-py3-none-any.whl (17 kB)\nInstalling collected packages: pretty-errors, torchmetrics, lightning\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.3.2\n    Uninstalling torchmetrics-1.3.2:\n      Successfully uninstalled torchmetrics-1.3.2\nSuccessfully installed lightning-2.2.3 pretty-errors-1.2.25 torchmetrics-1.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nimport pydicom\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ProcessPoolExecutor\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T08:17:21.169535Z","iopub.execute_input":"2024-05-11T08:17:21.169857Z","iopub.status.idle":"2024-05-11T08:17:21.175748Z","shell.execute_reply.started":"2024-05-11T08:17:21.169825Z","shell.execute_reply":"2024-05-11T08:17:21.174875Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_KEY\")\n\nwandb.login(key=secret_value_0)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:17:21.177901Z","iopub.execute_input":"2024-05-11T08:17:21.178899Z","iopub.status.idle":"2024-05-11T08:17:23.425131Z","shell.execute_reply.started":"2024-05-11T08:17:21.178871Z","shell.execute_reply":"2024-05-11T08:17:23.424115Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nimport pydicom\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ProcessPoolExecutor\n\nclass ImageConverter:\n    def __init__(self, input_folder, output_folder, csv_path, image_size=512):\n        self.input_folder = input_folder\n        self.output_folder = output_folder\n        self.csv_path = csv_path\n        self.image_size = image_size\n\n    def _convert_single_dcm_to_png(self, file_name, label):\n        # Check if the DICOM file exists\n        dicom_path = os.path.join(self.input_folder, file_name + '.dcm',)\n        if not os.path.exists(dicom_path):\n            print(f\"Warning: DICOM file not found for {dicom_path}\")\n            return\n\n        # Read DICOM file\n        dicom_data = pydicom.dcmread(dicom_path)\n\n        # Convert DICOM to PNG\n        image_array = dicom_data.pixel_array\n        image = Image.fromarray(image_array)\n        image = image.resize((self.image_size, self.image_size))\n\n        # Define the output path based on train/test/val and label\n        if self.index % 5 == 0:  # 20% for validation\n            output_path = os.path.join(self.output_folder, f'val/{label}/{file_name[:-4]}.png')\n        elif self.index % 5 == 1:  # 20% for test\n            output_path = os.path.join(self.output_folder, f'test/{label}/{file_name[:-4]}.png')\n        else:  # 60% for train\n            output_path = os.path.join(self.output_folder, f'train/{label}/{file_name[:-4]}.png')\n\n        # Save the image\n        image.save(output_path)\n\n    def _convert_dcm_to_png_for_index(self, index_row):\n        self.index, row = index_row\n        file_name = row['patientId']\n        label = row['Target']\n        self._convert_single_dcm_to_png(file_name, label)\n\n    def convert_dcm_to_png_parallel(self):\n        # Create output folders if they don't exist\n        for folder in ['train/0/', 'train/1/', 'test/0/', 'test/1/', 'val/0/', 'val/1/']:\n            os.makedirs(os.path.join(self.output_folder, folder), exist_ok=True)\n\n        # Read CSV file\n        df = pd.read_csv(self.csv_path)\n        print('Total files: ', df.shape[0])\n\n        # Use ProcessPoolExecutor for parallel processing\n        with ProcessPoolExecutor() as executor:\n            list(tqdm(executor.map(self._convert_dcm_to_png_for_index, df.iterrows()), total=len(df), desc=\"Converting images\"))\n# if __name__ == \"__main__\":\ninput_folder = \"/kaggle/working/rsna-pneumonia-detection-dataset/stage_2_train_images\"\noutput_folder = \"/kaggle/working/CycleGan-CFE/train-data\"\ncsv_path = \"/kaggle/working/rsna-pneumonia-detection-dataset/stage_2_train_labels.csv\"\n\nimage_converter = ImageConverter(input_folder, output_folder, csv_path)\nimage_converter.convert_dcm_to_png_parallel()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassifierDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n\n        self.classes = ['0', '1']\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n\n        self.samples = self._make_dataset()\n\n    def _make_dataset(self):\n        samples = []\n        for class_name in self.classes:\n            class_dir = os.path.join(self.root_dir, class_name)\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n                samples.append((img_path, self.class_to_idx[class_name]))\n        return samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        img = Image.open(img_path).convert('L')  # Convert to grayscale\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:20:47.947706Z","iopub.execute_input":"2024-05-11T08:20:47.948688Z","iopub.status.idle":"2024-05-11T08:20:47.957654Z","shell.execute_reply.started":"2024-05-11T08:20:47.948637Z","shell.execute_reply":"2024-05-11T08:20:47.956735Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport lightning as pl\nimport wandb\nfrom lightning.pytorch.loggers.wandb import WandbLogger\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.tuner import Tuner\nimport tqdm.auto as tqdm\nfrom torchmetrics import Accuracy\n\nclass Classifier(pl.LightningModule):\n    def __init__(self, transfer=True):\n        super(Classifier, self).__init__()\n        self.conv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)  # Adjust input channels to 3\n        self.model = models.swin_t(weights='IMAGENET1K_V1')\n        if transfer:\n            # layers are frozen by using eval()\n            self.model.eval()\n            # freeze params\n            for p in self.model.parameters() : \n                p.requires_grad = False\n        num_ftrs = 768\n        self.model.head = nn.Sequential(\n            nn.Linear(in_features=num_ftrs, out_features=256),\n            nn.LeakyReLU(),\n            nn.Dropout(p=0.5), \n            nn.Linear(in_features=256 , out_features=2),\n            nn.Softmax(dim=1)  \n        ) \n\n        self.criterion = nn.CrossEntropyLoss()\n        self.train_accuracy = Accuracy(task='binary')\n        self.val_accuracy = Accuracy(task='binary')\n\n    def forward(self, x):\n        x = self.conv(x)\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = self.criterion(outputs, labels)\n        self.log('train_loss', loss)\n        # Calculate and log accuracy\n        _, preds = torch.max(outputs, 1)\n        acc = self.train_accuracy(preds, labels)\n        self.log('train_acc', acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = self.criterion(outputs, labels)\n        self.log('val_loss', loss, prog_bar=True, sync_dist=True)\n        # Calculate and log accuracy\n        _, preds = torch.max(outputs, 1)\n        acc = self.val_accuracy(preds, labels)\n        self.log('val_acc', acc, prog_bar=True, sync_dist=True)\n        return loss\n    \n    def on_train_epoch_end(self):\n        self.train_accuracy.reset()\n\n    def on_validation_epoch_end(self):\n        self.val_accuracy.reset()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            },\n            'monitor': 'val_loss'\n        }\n\nwandb_logger = WandbLogger(project=\"CycleGAN-CFE\", name=\"swin_t-classifier-training\")\n# Define data transformations\nIMAGE_SIZE = 512\nBATCH_SIZE = 16\nEPOCHS = 20\n\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize image to 512x512\n    transforms.ToTensor(),          \n    transforms.Normalize(mean=[0.485], std=[0.229])  # Normalize image\n])\n\n# Define dataset paths\ntrain_dir = \"/kaggle/working/CycleGan-CFE/train-data/train\"\nval_dir = \"/kaggle/working/CycleGan-CFE/train-data/val\"\n\n# Create datasets\ntrain_dataset = ClassifierDataset(root_dir=train_dir, transform=transform)\nval_dataset = ClassifierDataset(root_dir=val_dir, transform=transform)\nprint(\"Total Training Images: \",len(train_dataset))\nprint(\"Total Validation Images: \",len(val_dataset))\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:20:49.454608Z","iopub.execute_input":"2024-05-11T08:20:49.455414Z","iopub.status.idle":"2024-05-11T08:20:52.001801Z","shell.execute_reply.started":"2024-05-11T08:20:49.455380Z","shell.execute_reply":"2024-05-11T08:20:52.000913Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total Training Images:  16739\nTotal Validation Images:  6046\n","output_type":"stream"}]},{"cell_type":"code","source":"# Instantiate the discriminator model\nclf = Classifier(transfer=True)\n\ncheckpoint_callback = ModelCheckpoint(\n     monitor='val_loss',\n     dirpath='/kaggle/working/CycleGan-CFE/models/',\n     filename='swin_t-epoch{epoch:02d}-val_loss{val_loss:.2f}',\n     auto_insert_metric_name=False,\n )\n# Set up PyTorch Lightning Trainer with multiple GPUs and tqdm progress bar\ntrainer = pl.Trainer(\n    devices=2,\n    accelerator=\"gpu\",\n    max_epochs=EPOCHS,\n    accumulate_grad_batches=10,\n    log_every_n_steps=1,\n    check_val_every_n_epoch=1,\n    benchmark=True,\n    logger=wandb_logger,\n    callbacks=[checkpoint_callback],\n)\n\n\n# Train the discriminator\ntrainer.fit(clf, train_loader, val_loader)\nwandb.finish()\n# if __name__ == \"__main__\":\n#     main()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 512","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:20:54.024064Z","iopub.execute_input":"2024-05-11T08:20:54.024842Z","iopub.status.idle":"2024-05-11T08:20:54.029462Z","shell.execute_reply.started":"2024-05-11T08:20:54.024804Z","shell.execute_reply":"2024-05-11T08:20:54.028082Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, train_N, train_P, img_res=(128, 128)):\n        self.root_dir = root_dir\n        self.train_N = train_N\n        self.train_P = train_P\n        self.img_res = img_res\n        self.transforms = transforms.Compose([\n            transforms.Resize(img_res),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])  # Assuming grayscale images\n        ])\n\n    def __len__(self):\n        return min(len(os.listdir(os.path.join(self.root_dir, self.train_N))),\n                   len(os.listdir(os.path.join(self.root_dir, self.train_P))))\n\n    def __getitem__(self, idx):\n        normal_path = os.path.join(self.root_dir, self.train_N, os.listdir(os.path.join(self.root_dir, self.train_N))[idx])\n        pneumo_path = os.path.join(self.root_dir, self.train_P, os.listdir(os.path.join(self.root_dir, self.train_P))[idx])\n        \n        normal_img = Image.open(normal_path).convert(\"L\")  # Load as grayscale\n        pneumo_img = Image.open(pneumo_path).convert(\"L\")  # Load as grayscale\n        \n        normal_img = self.transforms(normal_img)\n        pneumo_img = self.transforms(pneumo_img)\n        \n        return normal_img, pneumo_img\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:20:54.684888Z","iopub.execute_input":"2024-05-11T08:20:54.685550Z","iopub.status.idle":"2024-05-11T08:20:54.696825Z","shell.execute_reply.started":"2024-05-11T08:20:54.685514Z","shell.execute_reply":"2024-05-11T08:20:54.695616Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport lightning as pl\nimport wandb\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ResUNetGenerator(pl.LightningModule):\n    def __init__(self, gf, channels, dropout_rate=0.3):\n        super(ResUNetGenerator, self).__init__()\n        self.gf = gf\n        self.channels = channels\n        self.dropout_rate = dropout_rate\n\n        # Define the layers for the encoder\n        self.conv2d = nn.Conv2d(channels, gf, kernel_size=4, stride=2, padding=1, padding_mode='reflect')\n        \n        self.conv2d_layers_left = nn.ModuleList([\n            nn.Conv2d(gf * 2**i, gf * 2**(i+1), kernel_size=4, stride=2, padding=1, padding_mode='reflect')\n            for i in range(4)\n        ])\n        \n        self.conv2d_layers_right = nn.ModuleList([\n            nn.Conv2d(gf * 2**(i+1), gf * 2**i, kernel_size=3, stride=1, padding=1, padding_mode='reflect')\n            for i in range(4)\n        ])\n        \n        self.groupNorm_layers = nn.ModuleList([\n            nn.GroupNorm(8, gf * 2**(i+1))\n            for i in range(4)\n        ])\n        \n        self.res_blocks_left = nn.ModuleList([\n            ResidualBlock(gf * 2**(i+1), gf * 2**(i+1), dropout_rate)\n            for i in range(4)\n        ])\n        \n        self.res_blocks_right = nn.ModuleList([\n            ResidualBlock(gf * 2**i, gf * 2**i, dropout_rate)\n            for i in range(4)\n        ])\n\n        # Define the layers for the decoder\n        self.deconv2d_layers = nn.ModuleList([\n            nn.ConvTranspose2d(gf * 2**(4-i), gf * 2**(3-i), kernel_size=4, stride=2, padding=1)\n            for i in range(4)\n        ])\n        self.deconv2d_final = nn.ConvTranspose2d(gf, channels, kernel_size=4, stride=2, padding=1)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        self.group_norm = nn.GroupNorm(8, gf)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        \n        d0 = self.leaky_relu(self.group_norm(self.conv2d(x)))\n        d1 = self.leaky_relu(self.groupNorm_layers[0](self.conv2d_layers_left[0](d0)))\n        d1 = self.res_blocks_left[0](d1)\n        \n        d2 = self.leaky_relu(self.groupNorm_layers[1](self.conv2d_layers_left[1](d1)))\n        d2 = self.res_blocks_left[1](d2)\n        \n        d3 = self.leaky_relu(self.groupNorm_layers[2](self.conv2d_layers_left[2](d2)))\n        d3 = self.res_blocks_left[2](d3)\n        \n        d4 = self.leaky_relu(self.groupNorm_layers[3](self.conv2d_layers_left[3](d3)))\n        d4 = self.res_blocks_left[3](d4)\n\n\n        # Decoder\n        u1 = self.deconv2d_layers[0](d4)\n        u1 = torch.cat((u1, d3), dim=1)\n        u1 = self.leaky_relu(self.groupNorm_layers[2](self.conv2d_layers_right[3](u1)))\n        u1 = self.res_blocks_right[3](u1)\n        \n        u2 = self.deconv2d_layers[1](u1)\n        u2 = torch.cat((u2, d2), dim=1)\n        u2 = self.leaky_relu(self.groupNorm_layers[1](self.conv2d_layers_right[2](u2)))\n        u2 = self.res_blocks_right[2](u2)\n\n        u3 = self.deconv2d_layers[2](u2)\n        u3 = torch.cat((u3, d1), dim=1)\n        u3 = self.leaky_relu(self.groupNorm_layers[0](self.conv2d_layers_right[1](u3)))\n        u3 = self.res_blocks_right[1](u3)\n        \n        u4 = self.deconv2d_layers[3](u3)\n        u4 = torch.cat((u4, d0), dim=1)\n        u4 = self.leaky_relu(self.group_norm(self.conv2d_layers_right[0](u4)))\n        u4 = self.res_blocks_right[0](u4)\n\n        output_img = self.sig(self.deconv2d_final(u4))\n\n        return output_img\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:20:55.858626Z","iopub.execute_input":"2024-05-11T08:20:55.859464Z","iopub.status.idle":"2024-05-11T08:20:55.886663Z","shell.execute_reply.started":"2024-05-11T08:20:55.859426Z","shell.execute_reply":"2024-05-11T08:20:55.885765Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Discriminator(pl.LightningModule):\n    def __init__(self, df):\n        super(Discriminator, self).__init__()\n        self.df = df\n        # Define the layers for the discriminator\n        self.conv_layers = nn.ModuleList([nn.Sequential(\n            nn.Conv2d(1 if i == 0 else df * 2**(i-1), df * 2**i, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.GroupNorm(8, df * 2**i)) for i in range(4)])\n        \n        self.final_conv = nn.Conv2d(df * 8, 1, kernel_size=4, stride=1, padding=1)\n\n    def forward(self, x):\n        out = x\n        for conv_layer in self.conv_layers:\n            out = conv_layer(out)\n        validity = self.final_conv(out)\n        return validity\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))\n        return optimizer\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:20:57.264201Z","iopub.execute_input":"2024-05-11T08:20:57.264569Z","iopub.status.idle":"2024-05-11T08:20:57.273708Z","shell.execute_reply.started":"2024-05-11T08:20:57.264540Z","shell.execute_reply":"2024-05-11T08:20:57.272578Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nmodel = Discriminator(df=64)\n# Summarize the model architecture\nsummary(model, input_size=(1, 512, 512))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nmodel = ResUNetGenerator(gf=32, channels=1)\n# Summarize the model architecture\nsummary(model, input_size=(1, 512, 512))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 4","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:21:03.316016Z","iopub.execute_input":"2024-05-11T08:21:03.316410Z","iopub.status.idle":"2024-05-11T08:21:03.321864Z","shell.execute_reply.started":"2024-05-11T08:21:03.316377Z","shell.execute_reply":"2024-05-11T08:21:03.320555Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nclass CycleGAN(pl.LightningModule):\n    def __init__(self, img_shape=(1, 512, 512), gf=32, df=64, lambda_cycle=10.0, lambda_id=0.1, classifier_path=None, classifier_weight=None):\n        super(CycleGAN, self).__init__()\n        self.img_shape = img_shape\n        self.gf = gf\n        self.df = df\n        self.lambda_cycle = lambda_cycle\n        self.lambda_id = lambda_id * lambda_cycle\n        self.classifier_path = classifier_path\n        self.classifier_weight = classifier_weight\n\n        # Initialize the generator, discriminator, and classifier models\n        self.g_NP = ResUNetGenerator(gf, channels=self.img_shape[0])\n        self.g_PN = ResUNetGenerator(gf, channels=self.img_shape[0])\n        self.d_N = Discriminator(df)\n        self.d_P = Discriminator(df)\n        self.automatic_optimization = False\n        \n        self.classifier = Classifier()\n        checkpoint = torch.load(classifier_path)\n        self.classifier.load_state_dict(checkpoint['state_dict'])\n        self.classifier.eval()\n        self.freeze_classifier()\n    \n    def freeze_classifier(self):\n        print(\"freezing Classifier...\")\n        for p in self.classifier.parameters() : \n                p.requires_grad = False\n\n\n    def generator_training_step(self, img_N, img_P, opt):\n        self.toggle_optimizer(opt)\n        # Translate images to the other domain\n        fake_P = self.g_NP(img_N)\n        fake_N = self.g_PN(img_P)\n\n        # Translate images back to original domain\n        reconstr_N = self.g_PN(fake_P)\n        reconstr_P = self.g_NP(fake_N)\n\n        # Identity mapping of images\n        img_N_id = self.g_PN(img_N)\n        img_P_id = self.g_NP(img_P)\n        # Discriminators determine validity of translated images\n        valid_N = self.d_N(fake_N)\n        valid_P = self.d_P(fake_P)\n\n        class_N_loss = self.classifier(fake_N)\n        class_P_loss = self.classifier(fake_P)\n        # Adversarial loss\n        valid_target = torch.ones_like(valid_N)\n        adversarial_loss = nn.MSELoss()(valid_N, valid_target) + nn.MSELoss()(valid_P, valid_target)\n\n        # Cycle consistency loss\n        cycle_loss = nn.L1Loss()(reconstr_N, img_N) + nn.L1Loss()(reconstr_P, img_P)\n\n        # Identity loss\n        identity_loss = nn.L1Loss()(img_N_id, img_N) + nn.L1Loss()(img_P_id, img_P)\n\n        # Classifier loss\n        class_loss = nn.MSELoss()(class_N_loss, torch.ones_like(class_N_loss)) + nn.MSELoss()(class_P_loss, torch.zeros_like(class_P_loss))\n\n        # Total generator loss\n        total_loss = adversarial_loss + self.lambda_cycle * cycle_loss + self.lambda_id * identity_loss + self.classifier_weight * class_loss\n              \n        self.log('adversarial_loss', adversarial_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('cycle_loss', cycle_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('identity_loss', identity_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('class_loss', class_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('generator_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        \n        opt.zero_grad()\n        self.manual_backward(total_loss)\n        opt.step()\n        self.untoggle_optimizer(opt)\n        \n        return total_loss, adversarial_loss, cycle_loss\n\n    def discriminator_training_step(self, img_N, img_P, opt):\n        # Pass real images through discriminator D_N\n        self.toggle_optimizer(opt)\n        pred_real_N = self.d_N(img_N)\n        # Compute MSE loss for real Negative images\n        mse_real_N = nn.MSELoss()(pred_real_N, torch.ones_like(pred_real_N))\n\n        # Pass fake images from positive to discriminator D_N\n        fake_P = self.g_PN(img_P)\n        pred_fake_N = self.d_N(fake_P)\n        # Compute MSE loss for fake images in domain P\n        mse_fake_N = nn.MSELoss()(pred_fake_N, torch.zeros_like(pred_fake_N))\n        # Pass real images through discriminator D_P\n        pred_real_P = self.d_P(img_P)\n        # Compute MSE loss for real images in domain P\n        mse_real_P = nn.MSELoss()(pred_real_P, torch.ones_like(pred_real_P))\n\n        # Pass fake images from domain N to discriminator D_P\n        fake_N = self.g_NP(img_N)  # Detach to prevent backpropagation to generator\n        pred_fake_P = self.d_P(fake_N)\n        # Compute MSE loss for fake images in domain N\n        mse_fake_P = nn.MSELoss()(pred_fake_P, torch.zeros_like(pred_fake_P))\n        \n\n        # Compute total discriminator loss\n        dis_loss = 0.5 * (mse_real_N + mse_fake_N + mse_real_P + mse_fake_P)\n        opt.zero_grad()\n        self.manual_backward(mse_fake_P)\n        opt.step()\n        self.untoggle_optimizer(opt)\n        \n        self.log('mse_fake_N', mse_fake_N, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('mse_fake_P', mse_fake_P, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log('discriminator_loss', dis_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return dis_loss, mse_fake_N, mse_fake_P\n    \n    def training_step(self, batch, batch_idx):\n        img_N, img_P = batch\n        optD, optG = self.optimizers()\n        \n        total_loss, adversarial_loss, cycle_loss = self.generator_training_step(img_N, img_P, optG)\n        dis_loss, mse_fake_N, mse_fake_P = self.discriminator_training_step(img_N, img_P, optD) \n        \n        return {\"generator_loss\": total_loss, \"adversarial_loss\": adversarial_loss, \"cycle_loss\": cycle_loss, \"discriminator_loss\": dis_loss, \"mse_fake_N\": mse_fake_N, \"mse_fake_P\": mse_fake_P}\n    \n    def validation_step(self, batch, batch_idx):\n        img_N, img_P = batch\n\n        # Translate images to the other domain\n        fake_P = self.g_NP(img_N)\n        fake_N = self.g_PN(img_P)\n\n        # Translate images back to original domain\n        reconstr_N = self.g_PN(fake_P)\n        reconstr_P = self.g_NP(fake_N)\n\n        # Identity mapping of images\n        img_N_id = self.g_PN(img_N)\n        img_P_id = self.g_NP(img_P)\n\n        # Discriminators determine validity of translated images\n        valid_N = self.d_N(fake_N)\n        valid_P = self.d_P(fake_P)\n\n        class_N_loss = self.classifier(fake_N)\n        class_P_loss = self.classifier(fake_P)\n\n        # Adversarial loss\n        valid_target = torch.ones_like(valid_N)\n        adversarial_loss = nn.MSELoss()(valid_N, valid_target) + nn.MSELoss()(valid_P, valid_target)\n\n        # Cycle consistency loss\n        cycle_loss = nn.L1Loss()(reconstr_N, img_N) + nn.L1Loss()(reconstr_P, img_P)\n\n        # Identity loss\n        identity_loss = nn.L1Loss()(img_N_id, img_N) + nn.L1Loss()(img_P_id, img_P)\n\n        # Classifier loss\n        class_loss = nn.MSELoss()(class_N_loss, torch.ones_like(class_N_loss)) + nn.MSELoss()(class_P_loss, torch.zeros_like(class_P_loss))\n\n        # Total generator loss\n        total_loss = adversarial_loss + self.lambda_cycle * cycle_loss + self.lambda_id * identity_loss + self.classifier_weight * class_loss\n\n        self.log('val_adversarial_loss', adversarial_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_cycle_loss', cycle_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_identity_loss', identity_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_class_loss', class_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_generator_loss', total_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n\n        return total_loss\n\n    def configure_optimizers(self):\n        optG = torch.optim.Adam(itertools.chain(self.g_NP.parameters(), self.g_PN.parameters()),lr=2e-4, betas=(0.5, 0.999))\n        optD = torch.optim.Adam(itertools.chain(self.d_N.parameters(), self.d_P.parameters()),lr=2e-4, betas=(0.5, 0.999))\n        \n        gamma = lambda epoch: 1 - max(0, epoch + 1 - 100) / 101\n        schD = LambdaLR(optD, lr_lambda=gamma)\n#         Optimizer= [optD, optG]\n        return optD, optG\n\n    def train_dataloader(self):\n        root_dir = \"/kaggle/working/CycleGan-CFE/train-data/train\"\n        train_N = \"0\"\n        train_P = \"1\"\n        img_res = (image_size, image_size)\n\n        dataset = CustomDataset(root_dir=root_dir, train_N=train_N, train_P=train_P, img_res=img_res)\n\n        # Set up DataLoader for parallel processing and GPU acceleration\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n\n        return dataloader\n    \n    def val_dataloader(self):\n        root_dir = \"/kaggle/working/CycleGan-CFE/train-data/val\"\n        train_N = \"0\"\n        train_P = \"1\"\n        img_res = (image_size, image_size)\n\n        dataset = CustomDataset(root_dir=root_dir, train_N=train_N, train_P=train_P, img_res=img_res)\n\n        # Set up DataLoader for parallel processing and GPU acceleration\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n        return dataloader\n     \n\n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        if batch_idx % 100 == 0:\n            # Get a random batch from the test dataloader\n            batch = next(iter(test_dataloader))\n            img_N, img_P = batch\n\n            # Pick a random image from the batch\n            idx = np.random.randint(img_N.size(0))\n            img_N = img_N[idx].unsqueeze(0).to('cuda')\n            img_P = img_P[idx].unsqueeze(0).to('cuda')\n            # Translate images to the other domain\n            fake_P = self.g_NP(img_N)\n            fake_N = self.g_PN(img_P)\n\n            # Translate images back to original domain\n            reconstr_N = self.g_PN(fake_P)\n            reconstr_P = self.g_NP(fake_N)\n\n            # Plot the images\n            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n            # Plot real N, translated P, and reconstructed N\n            axes[0, 0].imshow(img_N.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n            axes[0, 0].set_title(\"Real N\")\n            axes[0, 0].axis('off')\n\n            axes[0, 1].imshow(fake_P.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n            axes[0, 1].set_title(\"Translated P\")\n            axes[0, 1].axis('off')\n\n            axes[0, 2].imshow(reconstr_N.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n            axes[0, 2].set_title(\"Reconstructed N\")\n            axes[0, 2].axis('off')\n\n            # Plot real P, translated N, and reconstructed P\n            axes[1, 0].imshow(img_P.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n            axes[1, 0].set_title(\"Real P\")\n            axes[1, 0].axis('off')\n\n            axes[1, 1].imshow(fake_N.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n            axes[1, 1].set_title(\"Translated N\")\n            axes[1, 1].axis('off')\n\n            axes[1, 2].imshow(reconstr_P.squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), cmap='gray')\n            axes[1, 2].set_title(\"Reconstructed P\")\n            axes[1, 2].axis('off')\n\n            # Log the figure in WandB\n            wandb.log({\"test_images\": wandb.Image(fig)})\n\n            plt.close(fig)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:22:22.928831Z","iopub.execute_input":"2024-05-11T08:22:22.929316Z","iopub.status.idle":"2024-05-11T08:22:22.988941Z","shell.execute_reply.started":"2024-05-11T08:22:22.929281Z","shell.execute_reply":"2024-05-11T08:22:22.987749Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LambdaLR\nimport itertools\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\ncyclegan = CycleGAN(gf=32, df=64, classifier_path='/kaggle/input/swin-tiny/pytorch/v0/1/swin_t-epoch00-val_loss0.35.ckpt', classifier_weight=1)\n\ncheckpoint_callback = ModelCheckpoint(dirpath=\"/kaggle/working/CycleGan-CFE/models\",\n                                      filename='cyclegan-epoch_{epoch}-vloss_{val_generator_loss:.2f}.ckpt',\n                                      monitor='val_generator_loss',\n                                      save_top_k=3,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      verbose=True,\n                                      mode='min')\n\ntestdata_dir = \"/kaggle/working/CycleGan-CFE/train-data/val\"\ntrain_N = \"0\"\ntrain_P = \"1\"\nimg_res = (image_size, image_size)\n\ntest_dataset = CustomDataset(root_dir=testdata_dir, train_N=train_N, train_P=train_P, img_res=img_res)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\nwandb_logger = WandbLogger(project=\"CycleGAN-CFE\", name=\"GAN-training\",log_model=\"all\")\n# Create the trainer\ntrainer = pl.Trainer(\n    accelerator=\"auto\",\n    max_epochs=2,\n    log_every_n_steps=1,\n    benchmark=True,\n    devices=\"auto\",\n    logger=wandb_logger,\n    callbacks= [checkpoint_callback]\n)\n\n# Train the CycleGAN model\ntrainer.fit(cyclegan)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T08:25:10.375474Z","iopub.execute_input":"2024-05-11T08:25:10.375894Z","iopub.status.idle":"2024-05-11T08:30:15.200589Z","shell.execute_reply.started":"2024-05-11T08:25:10.375862Z","shell.execute_reply":"2024-05-11T08:30:15.199330Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"freezing Classifier.\n","output_type":"stream"},{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20240511_082511-1pxj8b6d</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE/runs/1pxj8b6d' target=\"_blank\">GAN-training</a></strong> to <a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE' target=\"_blank\">https://wandb.ai/anindyamitra2018/CycleGAN-CFE</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/anindyamitra2018/CycleGAN-CFE/runs/1pxj8b6d' target=\"_blank\">https://wandb.ai/anindyamitra2018/CycleGAN-CFE/runs/1pxj8b6d</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working/CycleGan-CFE/models exists and is not empty.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name       | Type             | Params\n------------------------------------------------\n0 | g_NP       | ResUNetGenerator | 15.0 M\n1 | g_PN       | ResUNetGenerator | 15.0 M\n2 | d_N        | Discriminator    | 2.8 M \n3 | d_P        | Discriminator    | 2.8 M \n4 | classifier | Classifier       | 27.7 M\n------------------------------------------------\n35.5 M    Trainable params\n27.7 M    Non-trainable params\n63.2 M    Total params\n252.837   Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"467c1f9b54924932bd797b8de98332bc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n","output_type":"stream"}]},{"cell_type":"code","source":"pl.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}